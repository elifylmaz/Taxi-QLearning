# 6Ã—6 Taxi OrtamÄ±nda Optimize Q-Learning  

## ğŸ“Œ Ã–zet  
Bu Ã§alÄ±ÅŸmada klasik *Taxi-v3* ortamÄ±, 6Ã—6 boyutunda engeller iÃ§erecek ÅŸekilde yeniden tasarlanmÄ±ÅŸ, **action masking** uygulanmÄ±ÅŸ ve durum uzayÄ± sadeleÅŸtirilerek 622.080 durumdan **47.952** duruma dÃ¼ÅŸÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r.  
5 milyon episodluk Q-Learning eÄŸitimi sonucunda ajan **ortalama 12 adÄ±mda** gÃ¶revi tamamlamayÄ± Ã¶ÄŸrenmiÅŸ ve en iyi ortalama Ã¶dÃ¼l **8.99** olarak elde edilmiÅŸtir.

---

## 1. GiriÅŸ  
Taxi-v3 ortamÄ± kÃ¼Ã§Ã¼k yapÄ±sÄ± nedeniyle tabular yÃ¶ntemlerle kolay Ã§Ã¶zÃ¼lebilmektedir.  
Bu Ã§alÄ±ÅŸmada daha zorlaÅŸtÄ±rÄ±lmÄ±ÅŸ bir **6Ã—6 grid-world** tasarlanmÄ±ÅŸ, haritaya engeller ve duvarlar eklenmiÅŸ, baÅŸlangÄ±Ã§ koÅŸullarÄ± dinamik hale getirilmiÅŸtir.  

Bu tasarÄ±m ile hedef:  
- Action maskingâ€™in,  
- Durum uzayÄ± sadeleÅŸtirmenin  

tabular Q-Learning performansÄ±na etkisini incelemektir.

---

## 2. Ortam TasarÄ±mÄ±

### ğŸ—ºï¸ Harita Ã–zellikleri
- 6Ã—6 grid  
- **3 yasak hÃ¼cre**, **4 sabit duvar**
- Yolcu ve hedef her resetâ€™te rastgele belirlenir
- Pickup/dropoff tÃ¼m geÃ§erli hÃ¼crelerde yapÄ±labilir
- **Action masking**: duvara Ã§arpma ve geÃ§ersiz pickup/dropoff engellenir

### ğŸ® Aksiyonlar (6 adet)
- YukarÄ±  
- AÅŸaÄŸÄ±  
- Sol  
- SaÄŸ  
- Pickup  
- Dropoff  

### ğŸ¯ Ã–dÃ¼ller
- Normal adÄ±m: **â€“1**  
- GeÃ§ersiz hareket: **â€“10**  
- BaÅŸarÄ±lÄ± bÄ±rakma: **+20**

---

## 3. Durum UzayÄ±  
Yolcu taksideyken konumu ayrÄ±ca tutulmadÄ±ÄŸÄ± iÃ§in:

- **Yolcu dÄ±ÅŸarÄ±da:**  
  - 6Â² Ã— 6Â² = **46.656**  
- **Yolcu takside:**  
  - 6Â² Ã— 6Â² = **1.296**  

### âœ”ï¸ Toplam  
**47.952 durum**  
Bu sadeleÅŸtirme durum uzayÄ±nÄ± yaklaÅŸÄ±k **13 kat kÃ¼Ã§Ã¼ltmÃ¼ÅŸtÃ¼r.**

---

## 4. YÃ¶ntem  

- **Algoritma:** Q-Learning  
- **Hiperparametreler:**  
  - Î± = 0.1  
  - Î³ = 1.0  
  - Îµ = 0.1  
- **EÄŸitim SÃ¼reci:**  
  - 5.000.000 episod  
  - Her 50.000 episodda performans deÄŸerlendirme  
  - Action masking tÃ¼m aÅŸamalarda aktif  
- En iyi politika **best_q_table.npy** dosyasÄ±na kaydedildi  

---

## 5. EÄŸitim SonuÃ§larÄ±  

| Episod | Ortalama Ã–dÃ¼l | AdÄ±m |
|--------|---------------|------|
| 50.000 | â€“33.26 | 54.3 |
| 100.000 | +3.85 | 17.1 |
| 500.000 | +8.94 | 12.1 |
| 2.350.000 | +8.99 | 12.0 |
| 5.000.000 | +8.98 | 12.0 |

Ajan yaklaÅŸÄ±k **150k** episodda gÃ¶revi Ã§Ã¶zebilir hale gelmiÅŸ,  
**2.3â€“2.6 milyon** episod aralÄ±ÄŸÄ±nda en iyi performansa ulaÅŸmÄ±ÅŸtÄ±r.  

Toplam eÄŸitim sÃ¼resi: **1 saat 8 dakika 49 saniye**

---

## 6. Test SonuÃ§larÄ±  

| Senaryo | AdÄ±m | Ã–dÃ¼l |
|---------|------|-------|
| 1 | 25 | +8 |
| 2 | 19 | +12 |
| 3 | 25 | +8 |

Ajan tÃ¼m testlerde **engellere takÄ±lmadan**, yasak hÃ¼crelere girmeden ve **optimuma yakÄ±n rotalar** kullanarak gÃ¶revi baÅŸarÄ±yla tamamlamÄ±ÅŸtÄ±r.

---

## 7. SonuÃ§  
Bu Ã§alÄ±ÅŸmada iki temel geliÅŸtirme uygulanmÄ±ÅŸtÄ±r:

1. **Action masking**  
2. Yolcu taksiye bindiÄŸinde yolcu konumunun durumdan Ã§Ä±karÄ±lmasÄ±  

Bu iki iyileÅŸtirme sayesinde 6Ã—6 engelli Taxi ortamÄ±,  
**5 milyon episod sonunda ortalama 12 adÄ±mda Ã§Ã¶zÃ¼lebilir** hale gelmiÅŸ ve  
**en iyi ortalama Ã¶dÃ¼l 8.99** elde edilmiÅŸtir.

SonuÃ§lar, kÃ¼Ã§Ã¼k fakat etkili Ã¶n iÅŸlemlerin tabular Q-Learningâ€™i halen gÃ¼Ã§lÃ¼ ve uygulanabilir kÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶stermektedir.

---
